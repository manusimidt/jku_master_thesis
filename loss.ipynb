{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "from common.psm import psm_f_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_paper(similarity_matrix, metric_values, temperature, beta=1.0): \n",
    "    \"\"\"Contrative Loss with embedding similarity .\"\"\" \n",
    "    metricshape = tf.shape(metric_values)\n",
    "    ## z \\theta(X): embedding1 = nnmodel.representation(X) \n",
    "    # ## z \\theta(Y): embedding2 = nnmodel.representation(Y) \n",
    "    # ## similaritymatrix = cosinesimilarity(embedding1, embedding2 \n",
    "    # ## metricvalues = PSM(X, Y) \n",
    "    similarity_matrix /= temperature \n",
    "    neg_logits1 = similarity_matrix  \n",
    "\n",
    "    col_indices = tf.cast(tf.argmin(metric_values, axis=1), dtype=tf.int32) \n",
    "    pos_indices1 = tf.stack( \n",
    "        (tf.range(metricshape[0], dtype=tf.int32), col_indices), axis=1)\n",
    "    pos_logits1 = tf.gather_nd(similarity_matrix, pos_indices1)    \n",
    "\n",
    "    metric_values /= beta \n",
    "    similarity_measure = tf.exp(-metric_values)\n",
    "    pos_weights1 = -tf.gather_nd(metric_values, pos_indices1) \n",
    "    pos_logits1 += pos_weights1 \n",
    "    negative_weights = tf.math.log((1.0 - similarity_measure) + 1e-8)\n",
    "    neg_logits1 += tf.tensor_scatter_nd_update( \n",
    "          negative_weights, pos_indices1, pos_weights1)    \n",
    "    \n",
    "    neg_logits1 = tf.math.reduce_logsumexp(neg_logits1, axis=1) \n",
    "    return tf.reduce_mean(neg_logits1 - pos_logits1) # Equation 4  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code from repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google-research/google-research/blob/6574e2ca3fab2b76f08566709aae2721110a3b5d/pse/jumping_task/training_helpers.py#L97\n",
    "EPS = 1e-9\n",
    "def contrastive_loss_repository(similarity_matrix,\n",
    "                     metric_values,\n",
    "                     temperature,\n",
    "                     coupling_temperature=1.0,\n",
    "                     use_coupling_weights=True):\n",
    "    \"\"\"Contrative Loss with soft coupling.\"\"\"\n",
    "    logging.info('Using alternative contrastive loss.')\n",
    "    metric_shape = tf.shape(metric_values)\n",
    "    similarity_matrix /= temperature\n",
    "    neg_logits1, neg_logits2 = similarity_matrix, similarity_matrix\n",
    "\n",
    "    col_indices = tf.cast(tf.argmin(metric_values, axis=1), dtype=tf.int32)\n",
    "    pos_indices1 = tf.stack(\n",
    "        (tf.range(metric_shape[0], dtype=tf.int32), col_indices), axis=1)\n",
    "    pos_logits1 = tf.gather_nd(similarity_matrix, pos_indices1)\n",
    "\n",
    "    row_indices = tf.cast(tf.argmin(metric_values, axis=0), dtype=tf.int32)\n",
    "    pos_indices2 = tf.stack(\n",
    "        (row_indices, tf.range(metric_shape[1], dtype=tf.int32)), axis=1)\n",
    "    pos_logits2 = tf.gather_nd(similarity_matrix, pos_indices2)\n",
    "\n",
    "    if use_coupling_weights:\n",
    "        metric_values /= coupling_temperature\n",
    "        coupling = tf.exp(-metric_values)\n",
    "        pos_weights1 = -tf.gather_nd(metric_values, pos_indices1)\n",
    "        pos_weights2 = -tf.gather_nd(metric_values, pos_indices2)\n",
    "        pos_logits1 += pos_weights1\n",
    "        pos_logits2 += pos_weights2\n",
    "        negative_weights = tf.math.log((1.0 - coupling) + EPS)\n",
    "        neg_logits1 += tf.tensor_scatter_nd_update(\n",
    "            negative_weights, pos_indices1, pos_weights1)\n",
    "        neg_logits2 += tf.tensor_scatter_nd_update(\n",
    "            negative_weights, pos_indices2, pos_weights2)\n",
    "\n",
    "    neg_logits1 = tf.math.reduce_logsumexp(neg_logits1, axis=1)\n",
    "    neg_logits2 = tf.math.reduce_logsumexp(neg_logits2, axis=0)\n",
    "\n",
    "    loss1 = tf.reduce_mean(neg_logits1 - pos_logits1)\n",
    "    loss2 = tf.reduce_mean(neg_logits2 - pos_logits2)\n",
    "    return loss1 + loss2\n",
    "\n",
    "def cosine_similarity_tensor(x, y):\n",
    "  \"\"\"Computes cosine similarity between all pairs of vectors in x and y.\"\"\"\n",
    "  x_expanded, y_expanded = x[:, tf.newaxis], y[tf.newaxis, :]\n",
    "  similarity_matrix = tf.reduce_sum(x_expanded * y_expanded, axis=-1)\n",
    "  similarity_matrix /= (\n",
    "      tf.norm(x_expanded, axis=-1) * tf.norm(y_expanded, axis=-1) + EPS)\n",
    "  return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding is two-dimensional, n_states x contrastive_loss_head size\n",
    "np.random.seed(1)\n",
    "e1 = np.random.randint(low=0, high=255, size=(56, 64)).astype(np.float32)\n",
    "e2 = np.random.randint(low=0, high=255, size=(56, 64)).astype(np.float32)\n",
    "\n",
    "# Dimension is (n_states,)\n",
    "a1 = np.random.randint(0, 8, size=(56,))\n",
    "a2 = np.random.randint(0, 8, size=(56,))\n",
    "a2[10:20] = a1[30:40] # just make the action sequences partly similar\n",
    "\n",
    "temp = 0.1\n",
    "gamma = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1_tensor, e2_tensor = tf.convert_to_tensor(e1), tf.convert_to_tensor(e2)\n",
    "a1_tensor, a2_tensor = tf.convert_to_tensor(a1), tf.convert_to_tensor(a2)\n",
    "\n",
    "e1_torch, e2_torch = torch.from_numpy(e1), torch.from_numpy(e2)\n",
    "a1_torch, a2_torch = torch.from_numpy(a1), torch.from_numpy(a2)\n",
    "\n",
    "psm_matrix_torch = psm_f_fast(a1_torch, a2_torch)\n",
    "psm_matrix_tensor = tf.convert_to_tensor(psm_matrix_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(21.747622, shape=(), dtype=float32)\n",
      "tf.Tensor(8.222357, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sim_matrix_tensor = cosine_similarity_tensor(e1_tensor, e2_tensor)\n",
    "print(contrastive_loss_paper(sim_matrix_tensor, psm_matrix_tensor, temperature=temp))\n",
    "\n",
    "sim_matrix_tensor = cosine_similarity_tensor(e1_tensor, e2_tensor)\n",
    "print(contrastive_loss_repository(sim_matrix_tensor, psm_matrix_tensor, temperature=temp, use_coupling_weights=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.7476)\n",
      "tensor(8.2224)\n"
     ]
    }
   ],
   "source": [
    "from common.training_helpers import cosine_similarity, contrastive_loss_paper, contrastive_loss_repository\n",
    "\n",
    "sim_matrix_torch = cosine_similarity(e1_torch, e2_torch)\n",
    "assert np.allclose(sim_matrix_tensor.numpy(), sim_matrix_torch.numpy())\n",
    "\n",
    "print(contrastive_loss_paper(sim_matrix_torch.clone(), psm_matrix_torch, temperature=temp))\n",
    "print(contrastive_loss_repository(sim_matrix_torch.clone(), psm_matrix_torch, temperature=temp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
